#IT相关微博

原文：美IT专家预警：自动武器研发恐引发AI军备竞赛

参考消息网2018年9月4日报道 美国石英财经网站近日发表“新闻高峰之家”研究员、谷歌公司和“深层思维”公司前工程师安德烈亚斯·基尔希的文章《自动武器将成为不知疲倦的高效杀人利器，而且无法阻止》称，下一次世界重大军事冲突可能速战速决。血肉之躯的战士根本没有机会赢得胜利。无人机和机器人将打垮我们的防御系统，占领我们立足的土地。即使我们消灭部分无人机和机器人，更多无人机和机器人很快就会赶到并取而代之，再次训练有素地击败我们的还击。这并非遥不可及的反乌托邦幻想，而是即将成为现实的事实。

文章称，今年5月，谷歌员工为抗议公司帮助美军开发无人机人工智能能力而辞职。更近时日，2400名研究人员立誓绝不研制自动武器。很多人工智能研究人员和工程师不愿从事自动武器研发，因为他们担心自己的研发结果可能引发一场人工智能武器竞赛。

阻止自动武器诞生已无可能？我们如何制止此事发生？

首选就是达成类似于不扩散核武器条约的协议，禁止研制自动武器。否则，出于道德原因而自愿放弃自动武器研制的有关各方将处于绝对劣势。

这是因为自动武器比血肉之躯的战士拥有很多优势。比如，它们不会疲劳，精确度更高，反应速度更快，可在人类无法生存的环境下作战，比如长期滞留沙漠地带。它们无需多年训练和培养，可以大规模生产。最坏的情形是被摧毁或损坏，而不是被打死打伤，无人为之哀悼或是要求把它们的遗体从战场运回来。

文章称，如果使用自动武器，也更为容易向公众解释军事作战的理由。由于进攻方的人员损失可以最小化，所以军队可以保持低调。美国和欧盟最近在利比亚、叙利亚和也门的作战行动就集中使用了无人机、巡航导弹等。没有此类武器的作战方处于明显劣势，军人不得不与机器人作战。

但是，即使所有国家都签署国际条约，禁止研制自动武器，就像它们曾经就不扩散核武器所签的条约一样，那也不可能阻止自动武器的诞生。这是因为两种战争模式存在天壤之别。自动武器无法像核武器一样被限制。

文章称，1958年达成的不扩散核武器条约有两大特征决定了条约效果显著：第一个特征就是部署核武器的准备时间比较漫长，因此其他签约国完全有时间对违约行为做出反应，实施制裁；第二个特征就是有效核查。但是，自动武器恰恰相反。

文章称，首先，它们的准备时间非常短：现在已经有了可用来制造自动武器的各种技术，并且正在公开独立研制。比如，坦克和作战飞机拥有大量传感器和摄像头，可记录正在发生的一切，飞行员已经可以通过计算机重新解读驾驶指令，从而与所驾驶的飞机交流。只要与人工智能结合，它们立即就能变成自动武器。

随着政府和私人实体投入的资金增加，人工智能研究正在取得越来越迅速的进展。毕竟，人工智能机器人不需要地面训练。

这就导致很难进行有效核查。自动武器所需的大部分技术和研究成果并不只用于自动武器。此外，自动武器发现人工智能的难度大大超过发现制造核武器的难度。人工智能机器人可在任何数据中心训练：毕竟，它们不仅仅是代码和数据，而现如今代码和数据可以轻而易举不留痕迹地移动和隐藏。大部分训练可在任何云服务器的模拟条件下进行，而对于外部核查人员来说，运行这样一套模拟程序似乎与预测明天的天气预报完全没有区别。

未来不可避免，责任不能回避。

文章称，如果不具备这两大特征，条约就是一纸空文，毫无用处。签约国仍然继续公开研究通用技术，然后悄悄用于自动武器，几乎不可能被发现。它们深知其他国家很可能在做同样的事，放弃研制不是选项。


那么，我们该当如何？我们不能回避责任。自动武器不可避免。自动武器的进攻和防御用途都需要研究，我们必须发展并壮大威慑能力。不过，即使我们不能避免自动武器，我们也能防止自动武器用于平民，监督限制使用自动武器。

文章称，结局不会是幸福的，只能是我们去适应并接受。
责任编辑：张义凌

#What I am thinking

目前科技正以高速发展，很多不可控因素成了潜在的威胁。科技是把双刃剑的理论一直在漂浮，对于伦理道德的拷问也越发敏感。作为一名程序员我们的未来该何去何从？所有的问题是刚入门的我们应该思考的。

每个人都有属于自己的道德底线，区别在于有人能够坚守而有人不能，期待一个大同社会的想法并不现实，要求每一个人都尽善其美也不大可能。

我们能做的就是坚守自己的道德地底线。人为财死鸟为食亡，为了利益而做出一些有违道德的事时有发生。而我们要摒弃利益大过天的想法，秉持着为科技发展奋斗终身的想法，才不会在利益的河流中迷失了方向。精神的喜悦也许有时不被理解，精神的喜悦有时甚至无人分享，但精神的喜悦亘古不变，它会在时间的长河里沉淀，像一杯美酒，越经沉淀就越发显得醇厚。

推荐这篇微博是希望引起大家的思考，同时与君共勉，永葆初心。
